Claro, aqu√≠ est√° el texto traducido al espa√±ol:

# Comparaci√≥n de M√©todos de Ensamble: Boosting vs. Bagging

Este proyecto realiza un an√°lisis comparativo detallado de dos potentes t√©cnicas de ensamble, Bagging (Random Forest) y Boosting (AdaBoost y Gradient Boosting), para resolver un problema de predicci√≥n de altos ingresos.

-----

### üéØ **Objetivo Principal**

  - **Prop√≥sito:** Aplicar, evaluar y comparar el rendimiento de los modelos Random Forest, AdaBoost y Gradient Boosting en la misma tarea de clasificaci√≥n.
  - **Problema de Negocio:** Determinar el modelo m√°s efectivo e interpretable para predecir si una persona gana m√°s de $50,000 anualmente bas√°ndose en datos demogr√°ficos.

### üìä **Dataset Utilizado**

  - **Nombre:** Adult Income
  - **Fuente:** Cargado desde un archivo CSV local (`adult.csv`). Contiene datos demogr√°ficos y de empleo de la base de datos del Censo de 1994.

### üõ†Ô∏è **Metodolog√≠a y Modelos Aplicados**

El proyecto sigue un flujo de trabajo estructurado de aprendizaje autom√°tico:

1.  **An√°lisis Exploratorio de Datos (EDA):** An√°lisis inicial y visualizaci√≥n de variables clave para comprender la estructura del dataset.
2.  **Preprocesamiento Robusto:** Uso de un `ColumnTransformer` para manejar eficientemente tanto las caracter√≠sticas categ√≥ricas (con `OneHotEncoder`) como las num√©ricas (con `StandardScaler`).
3.  **Entrenamiento de Modelos:** Se implementaron tres modelos de ensamble:
      - **Bagging:** `RandomForestClassifier`
      - **Boosting:**
          - `AdaBoostClassifier`
          - `GradientBoostingClassifier`
4.  **Evaluaci√≥n de Rendimiento:** Los modelos se evaluaron en base a m√©tricas clave de clasificaci√≥n, incluyendo **Exactitud (Accuracy)**, **Reporte de Clasificaci√≥n** (Precisi√≥n, Recall, F1-Score) y **Matriz de Confusi√≥n**.

### üöÄ **Resultados y Hallazgos Principales**

  - **Superioridad de los M√©todos de Ensamble:** Los tres modelos de ensamble demostraron un fuerte rendimiento predictivo, superando significativamente a un modelo de referencia simple.
  - **Mejor Rendimiento üèÜ:** **Gradient Boosting** result√≥ ser el modelo ganador, alcanzando la **exactitud (accuracy) m√°s alta, de aproximadamente 87.4%**.
  - **Competidores Cercanos:** `Random Forest` y `AdaBoost` fueron altamente competitivos, con exactitudes de 86.1% y 86.4%, respectivamente, demostrando que las tres son t√©cnicas viables.
  - **Variables M√°s Predictivas:** En los modelos exitosos, caracter√≠sticas clave como **edad**, **ganancias de capital**, **estado civil** y **horas trabajadas por semana** mostraron consistentemente la mayor importancia en la predicci√≥n de los niveles de ingresos.

### üèÜ **Recomendaci√≥n Final**

Para esta tarea de predicci√≥n, se recomienda la implementaci√≥n del modelo **Gradient Boosting**. Proporciona una ventaja de rendimiento ligera pero clara sobre los otros dos m√©todos, asegurando la clasificaci√≥n m√°s precisa de los rangos de ingresos.

# [**Ir al Proyecto**](../Actividad_2_Comparativa_Regresion/Actividad_3_Boosting_Bagging/Actividad_3_Modulo_5_Comparaci√≥n_m√©todos_Boosting_y_Bagging_predicci√≥n_ingresos.ipynb)

-----

## ‚öôÔ∏è **C√≥mo Ejecutar el Notebook**

Para ejecutar este proyecto en tu entorno local, sigue estos pasos:

1.  **Aseg√∫rate de tener Python 3.8 o una versi√≥n superior.**
2.  **Clona o descarga este repositorio.** Aseg√∫rate de que el archivo del dataset `adult.csv` est√© en el mismo directorio que el notebook.
3.  **Instala las librer√≠as necesarias:**
    ```bash
    pip install numpy pandas matplotlib seaborn scikit-learn
    ```
4.  **Ejecuta el Jupyter Notebook:**
    `Actividad_3_Modulo_5_Comparaci√≥n_m√©todos_Boosting_y_Bagging_predicci√≥n_ingresos.ipynb`

---

[Volver al √≠ndice principal](../../README.md) | [Volver a Modelos Avanzados](../README.md) | [Actividad Siguiente ‚Üí](../Actividad_4_Validacion_Cruzada/README.md)
